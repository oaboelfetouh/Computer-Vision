{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pretrained models.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOjWOd1eKmfu72lFTchaUlw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNa6i04fe_AD"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        " There are 2 basic uses for the pre-trained models!\n",
        "```\n",
        "\n",
        "\n",
        "1st : use a trained model directly ro image recognition\n",
        "\n",
        "2nd : **Transfer learning** is to adapt an existing model to recognize new types of objects instead of starting from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih3e3yFplsr1"
      },
      "source": [
        "# Use a trained model directly ro image recognition VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nE9XLyufATk",
        "outputId": "18655085-1c01-4c12-8ce4-8e40bad200a5"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "from keras.applications import vgg16\n",
        "\n",
        "#creat an instance of the model\n",
        "model = vgg16.VGG16()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 4s 0us/step\n",
            "553476096/553467096 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZIuzoQtfFaa"
      },
      "source": [
        "#load the image to the memory\n",
        "img = image.load_img(\"bay.jpg\", target_size = (224, 224))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9vNBCYsiPPu"
      },
      "source": [
        "the size of the image need to match the number of the input nodes in the neural network\n",
        "\n",
        "For VGG16, images fed into the network need to be (224,224)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N_14xG8hJgJ"
      },
      "source": [
        "***caution: look at thses 2 ways to image-procesing***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fF9WKzBafKhQ",
        "outputId": "6a4dd828-720b-4f00-ff2f-23b0cba3670b"
      },
      "source": [
        "#convert the image to a numpy array\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "#add a forth dimaention, since keras expects a list of images WAY 1:\n",
        "x1 = np.expand_dims(x, axis = 0)\n",
        "\n",
        "x1.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XWizpZcg_dJ"
      },
      "source": [
        "***Got the right output***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxyioYnkfHnf",
        "outputId": "9d56c612-bdc1-4d22-ebfa-05daba237b18"
      },
      "source": [
        "#convert the image to a numpy array\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "#add a forth dimaention, since keras expects a list of images WAY 2:\n",
        "x2 = np.array(list(x))\n",
        "\n",
        "x2.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8k6OAQThEkp"
      },
      "source": [
        "***Got the wrong output***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZlKr3Olg2ic"
      },
      "source": [
        "#normailze the images's pixals value to a range of 0-1\n",
        "x = vgg16.preprocess_input(x1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjh54aXniqjo"
      },
      "source": [
        "#run the image through the deep neural network to make prediction\n",
        "prediction = model.predict(x)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z2dhyBFjGAX"
      },
      "source": [
        "This function returns a 1,000 element array, each element tells us how likely our picture contains each of the one hundered objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyJauVvmjZBL",
        "outputId": "72ea398b-0af4-4a8f-a800-b35f525c19a4"
      },
      "source": [
        "#vgg16 de-code prediction function to tell us the names of the objects\n",
        "#by default, it gives ut the 5 most likely objects \n",
        "\n",
        "predected_classes = vgg16.decode_predictions(prediction, top = 9)\n",
        "print(\"Top predictions for this image: \")\n",
        "\n",
        "for imagenet_id, name, likelihood in predected_classes[0]:\n",
        "  print('Predictions : {} - {:2f}'.format(name , likelihood))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top predictions for this image: \n",
            "Predictions : seashore - 0.423801\n",
            "Predictions : promontory - 0.242124\n",
            "Predictions : lakeside - 0.240130\n",
            "Predictions : breakwater - 0.049119\n",
            "Predictions : sandbar - 0.032311\n",
            "Predictions : dock - 0.002331\n",
            "Predictions : cliff - 0.002244\n",
            "Predictions : beacon - 0.002014\n",
            "Predictions : valley - 0.001612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMZ4CnWYlzwB"
      },
      "source": [
        "# Transfer learning (feature extraction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPFR_1zFl2X4"
      },
      "source": [
        "import joblib #used for domping the features as .dat file\n",
        "from pathlib import Path\n",
        "from keras.preprocessing import image\n",
        "from keras.applications import vgg16\n",
        "import numpy as np\n",
        "\n",
        "#path to folders with training data\n",
        "dog = Path()/\"dogs\"\n",
        "not_dog = Path()/'not_dogs'\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for img in not_dog.glob(\"*.png\"):\n",
        "  img = image.load_img(img) #load the img in the memory\n",
        "  img_array = image.img_to_array(img) #convert in into the array\n",
        "  images.append(img_array) #add to the list\n",
        "  labels.append(0) # add zero to tell it's not a dog\n",
        "\n",
        "for img in dog.glob(\"*.png\"):\n",
        "  img = image.load_img(img)\n",
        "  img_array = image.img_to_array(img)\n",
        "  images.append(img_array)\n",
        "  labels.append(1)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epR7UWxdh65x"
      },
      "source": [
        "#keras expects the input to be a numpy array, so \n",
        "x_train = np.array(images)\n",
        "\n",
        "#convert the labels to an array\n",
        "y_train_ = np.array(labels)\n",
        "\n",
        "#normalize the images array\n",
        "x_train = vgg16.preprocess_input(x_train)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icSWTl8Cjr5b"
      },
      "source": [
        "We wont convert the y_train to categorical, why?\n",
        "'cuz this is a binary classification, we only need one element (0 or 1) for each input image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuiGggjLlCjh"
      },
      "source": [
        "weights = 'imagenet', we tell keras that we wanna load the virsion with the neural network pretrained on the 'imagenet' dataset\n",
        "\n",
        "Since we only use it for featrue extraction, we wanna chop-off the last layer of the neural network \n",
        "include_top = 'False'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjPJKSG3j-N8"
      },
      "source": [
        "#load a pre-trained neural network to use as feature extractor\n",
        "pretrained_nn = vgg16.VGG16(weights = 'imagenet', include_top = False, input_shape= (64,64,3))\n",
        "feature_extracted = pretrained_nn.predict(x_train)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlHa9AMRmGyF"
      },
      "source": [
        "the feature_extracted array now contain all the features in our training images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLLFv7namhLB"
      },
      "source": [
        "# Transfer learning (New NN traied with the extracted features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Sq567g4mnwW",
        "outputId": "44c40d2a-2bfb-4487-990d-8db3bec6a8d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "\n",
        "x_train = feature_extracted[:40]\n",
        "y_train = y_train_[:40]\n",
        "\n",
        "x_test = feature_extracted[40:]\n",
        "y_test = y_train_[40:]\n",
        "\n",
        "\n",
        "x_train.shape[1:]\n",
        "#x_train.shape returns an array, we use the last 3 elements by indexing [1:]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 2, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwxCshA5njbj",
        "outputId": "13877d3a-3a0c-4233-e52e-05ae9f32581c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#create the model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Flatten(input_shape= x_train.shape[1:]))\n",
        "model.add(Dense(256, activation= 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "#compile the model\n",
        "model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics= ['accuracy'])\n",
        "\n",
        "#train the model\n",
        "history = model.fit(x_train, y_train, epochs = 10)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 13.3864 - accuracy: 0.4500\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 3.7283 - accuracy: 0.8750\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 3.5951 - accuracy: 0.9000\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.7610e-13 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 9.7185e-11 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 3.0179e-08 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.2602e-12 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 5.0400e-16 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 4.2011e-09 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.3834e-06 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlUgZKPQpC2m",
        "outputId": "53435eab-6139-4258-95a4-5bf04e1eb193",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "scores = model.evaluate(x_test,y_test)\n",
        "print('accuracy : ', scores[1])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step - loss: 2.3779e-09 - accuracy: 1.0000\n",
            "accuracy :  1.0\n"
          ]
        }
      ]
    }
  ]
}